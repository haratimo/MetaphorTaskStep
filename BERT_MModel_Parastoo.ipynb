{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORbIteFf1Y+aRxWgpPm8kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haratimo/EEG_Target2Target/blob/main/BERT_MModel_Parastoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1**. Load the CSV and Pre-trained BERT model:"
      ],
      "metadata": {
        "id": "9A5aRAsj0cdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B36FHW8qwpBj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv(\"622Metaphors_and_W1W2.csv\")\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Embed the Metaphors using BERT:"
      ],
      "metadata": {
        "id": "3ZcvvVk40t2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_metaphor(metaphor):\n",
        "    inputs = tokenizer(metaphor, return_tensors=\"pt\", truncation=True, padding=True, max_length=20)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs['last_hidden_state'].mean(dim=1).numpy()\n",
        "\n",
        "# Using \"Metaphor\" column to compute embeddings\n",
        "df[\"Embedding\"] = df[\"Metaphor\"].apply(embed_metaphor)\n"
      ],
      "metadata": {
        "id": "mp4Cghx1zTrh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Calculate Similarity for Comparative Analysis:"
      ],
      "metadata": {
        "id": "pr2I5ISD2G0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_similarity(embedding, reference_embedding):\n",
        "    return cosine_similarity(embedding.reshape(1, -1), reference_embedding.reshape(1, -1))[0][0]\n",
        "\n",
        "# For demonstration purposes, using the embedding of the first metaphor as a reference\n",
        "reference_embedding = df[\"Embedding\"].iloc[0]\n",
        "\n",
        "df[\"Similarity_to_Reference\"] = df[\"Embedding\"].apply(lambda x: compute_similarity(x, reference_embedding))\n"
      ],
      "metadata": {
        "id": "lMHcXshI1lyw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Save the Results:\n",
        "To save the embeddings and (if computed) similarity scores back to CSV:"
      ],
      "metadata": {
        "id": "DKPS9KH22WDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to keep the embeddings in the CSV, you'll need to convert them to a string representation\n",
        "# as CSVs can't directly store complex data types like lists or arrays.\n",
        "df[\"Embedding\"] = df[\"Embedding\"].apply(lambda x: ' '.join(map(str, x)))\n",
        "\n",
        "df.to_csv(\"MetaphorsW1W2_with_embeddings_622.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "EyksIVP62WWm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- BERT Embedding Function ---\n",
        "def embed_metaphor(metaphor):\n",
        "    \"\"\"\n",
        "    Embed the given metaphor using BERT.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(metaphor, return_tensors=\"pt\", truncation=True, padding=True, max_length=20)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs['last_hidden_state'].mean(dim=1).numpy()\n",
        "\n",
        "# --- Initialize BERT ---\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# --- Load Dataset ---\n",
        "df = pd.read_csv(\"MetaphorsW1W2_622.csv\")\n",
        "\n",
        "# --- Calculate Embeddings ---\n",
        "embeddings = []\n",
        "for _, row in df.iterrows():\n",
        "    metaphor = f\"{row['W1']} is a {row['W2']}\"  # Constructing the metaphor\n",
        "    embedding = embed_metaphor(metaphor)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Saving embeddings to the dataframe\n",
        "df[\"BERT_embeddings\"] = embeddings\n",
        "\n",
        "# Assuming you have 'zGoodness' as a column in df, let's compute the cosine similarity\n",
        "# between each metaphor embedding and average embedding of all metaphors.\n",
        "avg_embedding = np.mean(embeddings, axis=0)\n",
        "df[\"BERT_similarity_avg\"] = [cosine_similarity([emb], [avg_embedding])[0][0] for emb in embeddings]\n",
        "\n",
        "# Compute the correlation between BERT_similarity_avg and zGoodness\n",
        "correlation = df[\"BERT_similarity_avg\"].corr(df[\"zGoodness\"])\n",
        "print(f\"Correlation between BERT-based similarity (to average) and zGoodness: {correlation:.2f}\")\n",
        "\n",
        "# Saving the updated dataframe with the computed scores\n",
        "df.to_csv(\"MetaphorsW1W2_with_scores_622.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "MTaLAkQv42kO",
        "outputId": "6db4b3f8-a809-4d4b-e34f-46be30657771"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-047719214ef2>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Assuming you have 'zGoodness' as a column in df, let's compute the cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# between each metaphor embedding and average embedding of all metaphors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mavg_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BERT_similarity_avg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mavg_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}